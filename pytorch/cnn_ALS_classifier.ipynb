{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import json\n",
    "import os\n",
    "import datetime\n",
    "# pip install torch==1.0.0 torchvision==0.2.1 -f https://download.pytorch.org/whl/cu80/torch_stable.html\n",
    "# pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu80\n",
    "# https://download.pytorch.org/whl/cu80/torch-1.0.0-cp35-cp35m-win_amd64.whl\n",
    "# pip install torch -f https://download.pytorch.org/whl/cu80/torch-1.0.0-cp35-cp35m-win_amd64.whl\n",
    "# pip install torch -f /d/3.Install/torch-1.0.0-cp37-cp37m-win_amd64.whl (success)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "# pip install torchsummary\n",
    "# from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# torch.version.cuda\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "file = open(\"config.json\")\n",
    "config = json.load(file)\n",
    "# print(config.keys())\n",
    "print(config['data']['frame_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = 44100              # Hz, sampling frequency\n",
    "f_size = config['data']['frame_size']\n",
    "h_size = config['data']['hop_size']\n",
    "n_mel_channels = config['data']['n_mel_channels']\n",
    "\n",
    "supra_frame_duration = 0.5  # seconds\n",
    "supra_frame_length = round(supra_frame_duration / (h_size/fs)) # need to read from config.json / \n",
    "\n",
    "dir_HC = '../wav'+ os.sep + 'HC' + os.sep\n",
    "dir_ALS = '../wav' + os.sep + 'ALS' + os.sep\n",
    "directory_ALS = os.listdir(dir_ALS)\n",
    "directory_HC = os.listdir(dir_HC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-analysis (transform wav-file to mel-spectrogramms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((20000,n_mel_channels, supra_frame_length))   # data\n",
    "y = np.zeros((20000,1))                     # labels\n",
    "X_global_index = 0\n",
    "X_ids = list([])\n",
    "# Cycle for HC (helthy controls)\n",
    "for i in range(len(directory_HC)):    \n",
    "    if directory_HC[i][-5:]=='a.wav':\n",
    "        # print(directory_HC[i][-5:])\n",
    "        fileName = dir_HC + directory_HC[i]\n",
    "        x, sr = librosa.load(fileName, sr=None, mono=True, offset=0.0, duration=None)\n",
    "\n",
    "        X_melspec = librosa.feature.melspectrogram(y=x, sr=sr, n_mels=64, n_fft=f_size, hop_length=h_size, window='hann', center=False, power=1.0)\n",
    "        X_melspec = 20*np.log10(X_melspec)\n",
    "            \n",
    "        frame_num = X_melspec.shape[1]    \n",
    "\n",
    "        X_melspec = X_melspec.astype(np.half)\n",
    "        X_melspec.tofile(dir_HC + directory_HC[i][:-4] +'_mel.dat')\n",
    "\n",
    "# #       #Запись фрагметов спектра в трехмерный массив \n",
    "#         N_supra_frames = frame_num - supra_frame_length + 1\n",
    "\n",
    "#         # generator comprehension\n",
    "#         gen_range = (n * 2 for n in range(0, np.floor(N_supra_frames/2).astype(int)))\n",
    "#         for n in gen_range:\n",
    "#             X[X_global_index,:,:] = X_melspec[:,n:n+supra_frame_length]\n",
    "#             X_ids.append(int(directory_HC[i][0:3]))\n",
    "#             y[X_global_index] = 0\n",
    "#             X_global_index = X_global_index +1\n",
    "    \n",
    "# Cycle for ALS (pathology)\n",
    "for i in range(len(directory_ALS)):\n",
    "    if directory_ALS[i][-5:]=='a.wav':\n",
    "        fileName = dir_ALS + directory_ALS[i]\n",
    "        x, sr = librosa.load(fileName, sr=None, mono=True, offset=0.0, duration=None)\n",
    "\n",
    "        X_melspec = librosa.feature.melspectrogram(y=x, sr=sr, n_mels=64, n_fft=f_size, hop_length=h_size, window='hann', center=False, power=1.0)\n",
    "        X_melspec = 20*np.log10(X_melspec)\n",
    "            \n",
    "        frame_num = X_melspec.shape[1]\n",
    "     \n",
    "        X_melspec = X_melspec.astype(np.half)\n",
    "        X_melspec.tofile(dir_ALS + directory_ALS[i][:-4] +'_mel.dat')\n",
    "\n",
    "# #       #Запись фрагметов спектра в трехмерный массив \n",
    "#         N_supra_frames = frame_num - supra_frame_length + 1\n",
    "\n",
    "#         # generator comprehension\n",
    "#         gen_range = (n * 2 for n in range(0, np.floor(N_supra_frames/2).astype(int)))\n",
    "#         for n in gen_range:\n",
    "#             X[X_global_index,:,:] = X_melspec[:,n:n+supra_frame_length]\n",
    "#             X_ids.append(int(directory_ALS[i][0:3]))\n",
    "#             y[X_global_index] = 1\n",
    "#             X_global_index = X_global_index + 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ALSDataset(Dataset):\n",
    "    def __init__(self, dir_HC_path, dir_ALS_path):\n",
    "        \"\"\"\n",
    "        Аргументы:\n",
    "            dir_HC_path -- путь к папке с записями здоровых голосов\n",
    "            dir_ALS_path -- путь к папке с записями голосов пациентов с БАС\n",
    "        \"\"\"\n",
    "        # self.dir_HC_path = []\n",
    "        # self.dir_ALS_path = []\n",
    "        self._ids = list([])    # speaker IDs\n",
    "        # 5-fold CV: 13+13+13+13+12\n",
    "        # ALS: 8,20,21, 22, 24, 25,27,28,31,32,39, 42,46,48,52,55,58,62,64,68,72,76,78,80,84,92,94,96,98,100,102\n",
    "        # HC : 131, 129, 127, 125, 123, 121, 119, 117, 115, 113, 111, 109, 107, 99, 97, 89, 86, 85, 81, 77, 65, 63, 61, 53, 49, 42, 28, 26, 24, 16, 6, 4, 2 \n",
    "        \n",
    "                                                                                        # HC men+women / ALS men+women\n",
    "        self.folds = list(([26,   97, 117,  85,  53,  86, 127,  92, 102,  27,  32,  72,  39], # 3+4 HC / 3+3 ALS\n",
    "                           [28,  123,  63,  99,  81, 125, 129,  68,  21,  55,  48,  78,  42], # 3+4 HC / 4+2 ALS\n",
    "                           [61,   16, 115, 109, 111,   2, 107,  46,  84,  64,  52,  24,  76], # 2+5 HC / 4+2 ALS\n",
    "                           [77,    6,  65,  24, 131, 119,  25,  94,  62,  28,   8,  98,  22], # 3+3 HC / 5+2 ALS\n",
    "                           [49,   89,   4,  42, 113, 121,  96,  20,  58,  80,  31, 100]))     # 2+4 HC / 4+2 ALS\n",
    "\n",
    "        f_size = 512            # need to read from config.json\n",
    "        h_size = 256            # need to read from config.json\n",
    "        n_mel_channels = 64     # need to read from config.json\n",
    "        supra_frame_length = 86 # need to read from config.json\n",
    "        self.X = np.zeros((20000, n_mel_channels, supra_frame_length))   # data\n",
    "        self.y = np.zeros((20000,1))                         # labels\n",
    "        X_global_index = 0\n",
    "        self.X_ids = list([])\n",
    "\n",
    "        # Cycle for HC (helthy controls)        \n",
    "        directory_HC = os.listdir(dir_HC_path)       \n",
    "        for i in range(len(directory_HC)):\n",
    "            if directory_HC[i][-9:]=='a_mel.dat':\n",
    "                fileName = dir_HC + directory_HC[i]\n",
    "            \n",
    "                X_spec = np.fromfile(dir_HC + directory_HC[i], dtype=np.half)\n",
    "                X_spec = np.reshape(X_spec,(n_mel_channels, np.floor(len(X_spec)/n_mel_channels).astype(int)))        \n",
    "\n",
    "                #Запись фрагметов спектра в трехмерный массив \n",
    "                frame_num = X_spec.shape[1]\n",
    "                N_supra_frames = frame_num - supra_frame_length + 1\n",
    "\n",
    "                # generator comprehension\n",
    "                gen_range = (n * 2 for n in range(0, np.floor(N_supra_frames/2).astype(int)))\n",
    "                for n in gen_range:\n",
    "                    self.X[X_global_index,:,:] = X_spec[:,n:n+supra_frame_length]\n",
    "                    self.X_ids.append(int(directory_HC[i][0:3]))\n",
    "                    self.y[X_global_index] = 0\n",
    "                    X_global_index = X_global_index +1\n",
    "\n",
    "        # Cycle for ALS (pathology)\n",
    "        directory_ALS = os.listdir(dir_ALS_path)       \n",
    "        for i in range(len(directory_ALS)):\n",
    "            if directory_ALS[i][-9:]=='a_mel.dat':\n",
    "                fileName = dir_ALS + directory_ALS[i]        \n",
    "\n",
    "                # print(fileName)\n",
    "\n",
    "                X_spec = np.fromfile(dir_ALS + directory_ALS[i], dtype=np.half)\n",
    "                X_spec = np.reshape(X_spec,(n_mel_channels, np.floor(len(X_spec)/n_mel_channels).astype(int)))\n",
    "\n",
    "        #       #Запись фрагметов спектра в трехмерный массив\n",
    "                frame_num = X_spec.shape[1]\n",
    "                N_supra_frames = frame_num - supra_frame_length + 1\n",
    "\n",
    "                # generator comprehension\n",
    "                gen_range = (n * 2 for n in range(0, np.floor(N_supra_frames/2).astype(int)))\n",
    "                for n in gen_range:\n",
    "                    self.X[X_global_index,:,:] = X_spec[:,n:n+supra_frame_length]\n",
    "                    self.X_ids.append(int(directory_ALS[i][0:3]))\n",
    "                    self.y[X_global_index] = 1\n",
    "                    X_global_index = X_global_index + 1\n",
    "            \n",
    "        inds_to_delete = np.array(range(self.y.shape[0]))>=X_global_index\n",
    "        self.y = np.delete(self.y,inds_to_delete)\n",
    "        self.X = np.delete(self.X,inds_to_delete, axis=0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index,:,:], self.y[index]\n",
    "\n",
    "    def get_kth_fold_inds(self, fold_num):\n",
    "        ids_train = list([])\n",
    "        ids_test = list([])\n",
    "        for i in range(len(self.X_ids)):\n",
    "            if self.X_ids[i] in self.folds[fold_num]:\n",
    "                ids_test.append(i)\n",
    "            else:\n",
    "                ids_train.append(i)\n",
    "        return (ids_train, ids_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label =  0.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAABCEUlEQVR4nO2de5Rd1X3fv1saaRg9htFIaCz08BWIlyKwMGODwfYS+IWJsUlqEyexQxNa2q44trOSleK2q2lW0664q43jtl5pWH4EJ46NTYKNXccPMNiOHziSUZCMeGtAEkICSZcZIVmjkXb/uFeavT/nzvnN1ePOHPr7rMXi/uace87v7L3P0d3f8/v9dogxynEcx6keM6baAcdxHOfE8Ae44zhORfEHuOM4TkXxB7jjOE5F8Qe44zhORenq5MlCmBelhU1rNrbSPgT7COxR2LyUNLqm2/guI3F4rGDYPP5B4/jptVjf5b+xbCfr2gh9L9vGNud2ttNh2GNtnFvKr437sh14bMI2fwn2HNhHDTv17QC29cBmO5wBm2N7JmxeO/s0vXZeB89NrD7ubfN46XYei+PDGk9sB/bxLMOmr2kf8txsN/Ypfed44Ll4X7LP02uhL4TnkqStL8QYz+JfO/oAbzy8b21+XoZtr4T9BOxh2Dtg98NOG2wVtg2V7NvqWGwmDhwefxNsdvZIG9/lw2Gpce6nYVu+l+271/CF7cQ+4fet4VZLPtNPnpvHJmzzn8C+GDb/4aRdSz7/FNsugr3L2P44bD40ee3bYafXfolxbsI+GIH95jaPl/Y571GOD56LvrAd2McDsHkvcPylD2We+9Ww2af0neOB7UJf2Od7J/jcCp5Lkt7HG1uSSyiO4ziVxR/gjuM4FSV0MhMzhMui9OOG0YNp4lzs/AKnY5xCD8GulWxfiG3Uuzj14rQU0/EeTvXAz2FHTplTf9rRGKXiNNPwxZIhU+IQ/lArP3ZhKshpJ+G1lMg57G5LfeEMmfJq3II/zIdNSY/7p9N3XCcPRV9MeYbtQqmAEkq6nW34EOwabIyX5dhchz1CXzmWz0s+s9PoN6VK+sZjo08Cjs+vb4WdnZ+atyWR7IFN3437sod9WHIqHqurxX0xFjbEGAf5Z/8F7jiOU1H8Ae44jlNR/AHuOI5TUTocRhh0XLPjmSmfFYQi6K1d56mUsWR7IfwY+lek/gnmwznuvh/2TvpSFhJn6cYGBf0VOuIY2on7p808xlAo6nzok/nwnXppD9rtYFkII+gzXLFCjilZzkdY1wj1WYL90zHE10Z9sBmaf4iDm+2CzWOtwshSknYsaP0cT8Y7EiucvnBjUttNtxcuBLY11nljIVQv4tw/Lzw0QPquCfsWQvfxhzFDry+0+1BuczwuSj7zHclLxvtASXqhxd/kv8Adx3Eqiz/AHcdxKsqkJJQQQp+kT0pao8YE8rckPSrpDjXmFkOSbowx7pv0mTmNoD1/oHw7p0AjQ/hDbeJzcwq8CDanKzz3c7BfAbswLS2bdgJKHMRqNxnSEl1JZ6lm+BumeiPUNXBdVoQkZbGQTLE5MuuGa2xS9kGhTzhdZ0gknD8jaThLKWBo3oM8N+UbhrnyYuhr4kBsY1+pGIpXL9+9KBVwAKVfsOQWhgZT3inLflXxPqVUWZATE3+sJx2T1Hcamb8FqQrjpa/kXPSzDpsRjCVM9hf4xyV9I8Z4oaRXqREke6uke2OM50m6V+M58o7jOE4HMB/gIYQzJb1R0qckKcY4GmOsS3qXpNubu90u6YbT46LjOI7TislIKCslPS/pMyGEV0naIOlDkgZijMcmMc+pOF+SJIUQbpF0S8NYIc1rbihIJrCtF/GctvbUJv5+NKaRbQRHSJL24XhjOJ6VmZdOS60CbIV2aDP7cSVsJp1mI4ANgZMvx3Vu48kwjeS1LMDxfw7f09NN8Nb9OFYbW9sXwN6HiIeAKJSy8cipfL1kX0kFyYTjkeO1ixESVvRFCo+N8XMQfcDxshV9ejH6cFNyfEocVvRNQbJr57omQeoPx9MYJTKO/ZJMSkkFOShAupyXm9mYYNRa4VnW4nQTjL/JSChdapTu+vMY46Vq1OXM5JLYyMdvmZMfY7wtxjgYYxxUKFRDdBzHcU6QyTzAt0vaHmN8oGnfqcYDfVcIYYkkNf+/+/S46DiO47TCfIDHGJ+TtC2EcEHzT2+S9LCkuyXd1PzbTZK+clo8dBzHcVoyqWqEIYS1aoQRzpb0lKTfVOPh/0VJK9RYReDGGCmw8TiDUVrftAwt91Js5joHBbm2JAvQ0tOpOR2EPjYfJ6OG1Qe7DpuaZqafQWtbUnQvY6elj2J/SwtOz8ewvxegC3fB10JoXtl1SsVOQ59nmqVRuZChettYYN9aTKDNDNiyTMxCFih9QbtZC/hYmmg21vll49xWZqZZWZHab3I86r78Ltc4YDQlw1ID+pzRltS1y7K7R0r8liYQgNsB1Su7cLFjyfkX4NyF4GveN5J0ZstqhJOKA48xbpRU+LIav8Ydx3GcKcAzMR3HcSpKh4tZJWccM6awlEysdUDJwVSiwZSkq5bbXIChkHFohNNx6kj2McwwsSnHcEbcx4PhWAzs4VqqnGZyCc0UrrW7ANPvfcZ0fTmyAAvZbKBUKsBGFrkvhBmWZQhKhTGwBOOP2bUR1zYvOf4I5L8jDMWzQtDAVsoe8PVgLbfT8TPCNjYWAqZEYhUBK2RiUnpI2mLEWCRhC3wtFJlj8SqMpzp2t+SeTJLBhRUkEy6EwUVdCMcXJRPun7QbQ4dJVwuZa4KiY/4L3HEcp6L4A9xxHKei+APccRynonRWA5+hcZ2Tabes6LcRtiX1cdGFfUOJAX2qD9+lnso065qRQs5rYao9NfN68pmaNSuRWQsX8NxPwGbsENO+U4m0jm19sEegp45Bo2Q7WpUVGRaWSqAr0Wb05UEjzJDjYaSW2wXNm87heOkiDSNYFLsP2i/HJt9zFN4lQPOej7Rs9nmm9TLkDL4UdGZwIWz6SoZgz0vOx3dJfeiDQpVP+m4srMKFLnhfst23JH24CO1SeIcCzbug/TPsGYOb4b+Ftig7NzAX2RjHf4E7juNUFH+AO47jVJRJZWKespPNHIya28zEZDgbJZQfw+6DTenhkZITW5dIeYa+cKpvZUv+CDbD8/qSz4xGsoq90zcsgam3w+Z0jO22Mfm8FtvYplyr70nYNdgM62I7lIU00k9e50uwKQ0xU5NS0l2wWYWPEkvqz7nYRtmL29lOvDbafSonDUP7AbZxWVNWn+T4sSQUjs+yxQZYufB6yFB/j/259kgdNn3l9stgD8FOfaWk8TxsPgO4rqmFJRcum+CzVLzPCguASFJomYnpv8Adx3Eqij/AHcdxKkpno1COanxabU3t+2AXsiUNOP1KoRTAyI2tsJlpyVbj9wtvmTG1fH0ytRzCrrQ5hWU7sR3Xw67BpjTQl3z+Krax+BAjYnhutnkfbPpWKJSUwDbksYb4BbRxnREzJeeSijIZp+ePlvjCGl3Mjh0yzs0pMyW6sj6nLxyLhO1AKaEGmxINpaqyBUHoC2Ut1H8qwPusBptyzkbYqTt8fliFsfpgs93qhi98ZqRSFb/LczOyrNU+TfwXuOM4TkXxB7jjOE5F8Qe44zhORel8NcJjWAt7UocuVE1jFhcqeKXHL9NapRZZV7DrsKntsfjcJmbawYFvJFlh1MqoI2/9aW7vfDV2QDvsRDtYmvkTJds2w7YWYGaYWAG0ywZopmuSz22PTKOC5D1GJbyNsBn6lxbd34Q2X2m0+RDsPthc2WAnG7bGL4zDdios1gyb460sLFCSdiJ+k1miWcU/9IGlcRcwFqPg8Wqw+Y4mdYeZlC8YlRO5cLQVXincp8twn6bvnqjHb4NvYfKLjfgvcMdxnIriD3DHcZyKMnUSCqfjdWP/QhhYYVHM3MwK3Rvrb9awmZILp0tvhl2nb5hOsVBOTKfgmH6zUNZ8TMVGeGxst4oXcSqY2uZ6iMSQsRgO95yxxmEaokZfmL12MexC9hqmwMw4fBBjggWpCmFbyfx9vlG8n31o+joHtlGQKpX4rPA2QhmC2bB9/AIlE2xOF9qgL3y6sE+ttSC3YbxchHZh9uRa2FnBM0OW4NqyVqZmQdLFfVjH9rSdC6GeuO5aC18pKTfxX+CO4zgVxR/gjuM4FcUf4I7jOBWlwxp41HgoGfSt7dyXGDo29dadqa5khAxtNRbfpXa3ETY1rQWX53Yd22MSuzXCBUwRarefWj/0MqbdUru13jXUks/UbunLxfDlQYZ9lYRySsXwTF5aeroJNL/jsCRAIXwOffzgEHao5WZh0eySRWu5qQ82j1UoA8GaBdfDxs0Q2VAJ8xFqN4I+Cdi+Ft/nuwXeh7yv6rCzxcO5sAUaqs77jPc0yxPi+0PYXAjHg519n22IC+XiJIVO7ik1C2Od5TrS7X3YxpDXIU0a/wXuOI5TUfwB7jiOU1EmJaGEEIbUCCA6ImksxjgYQuiXdIcac9EhSTfGGAuBQRkzgjS3OZWh7GBmlBmSSSFkKZnOW+FwlgzxDthGomVhsQFmv40k0zWee1bJdFmSdtZym/IOp3aczjOcLp1JMgxsEXwpTFE55eX3yzcX+iwtV8/KhXXYVobpAq6XiNA/Sg0LuaYq7NRXZgFvNzIxC1l7kEwK8h/3pwSYXCwXHhjZlNvzIKGwDzk28fVCRcqdGPw9iW/s721oQ0po83FdlBKW4PuUJZhFyrFfkCfTc0My4fg5iIOxD7m/ocJm7crbhmGpZvjuOO38Ar86xrg2WRXiVkn3xhjPk3Rv03Ycx3E6xMlIKO+SdHvz8+2SbjhpbxzHcZxJM9kolCjpWyGEKOkvYoy3SRqIMR6b7D2nYkknSVII4RZJt0iSuleMT5MpU3DqxmkE1ywknLamU0UWkufUrg/b67ApmWyAze8XMq0w/1qZnN+aLvHYXbXy/bcZ01BSliFmFannoZnVx2kmr5UjJpVzWNSLEQdWgX6uBcmRPoKTU/bitaRtsQxTc7YLIznYDjw274X9xvR+bcmCIEwTpsTCdUl57LJiZ5LUA1ktHW7WdXI8MfBjCAOK9zTHGwVbrhiZLsLBc3F88TopBzF7luOJ10oZbG3ymcFb7KNW0uMEUVmTfYC/Psa4I4SwWNK3QwjZMpwxxth8uBdoPuxvk6TQO9i5FZQdx3Fe5kxKQokx7mj+f7ca63m/VtKuEMISSWr+f/fpctJxHMcpYj7AQwhzQwjzj32W9FY1KkXfLemm5m43SfrK6XLScRzHKTIZCWVA0l0hhGP7/02M8RshhH+U9MUQws2SnpZ0o3mkwxrPoKOGyXAlZtpZixpz/zRjscvIduQCuITF/al5FqrLEYhe22rjn6nrUQ/ldVP3o142Hwek1rdRE28vhKShnUbQTqySx9HE7DSGPPLdRHrtPJalK7cbKlqHbS1UncI2fQQ2+4jvb74Pm9UK2e6hJGuZbcw+oeZN6rCtipRsx4NJdUwuNsJj8b5haB7HH3XfN8Bmn/E+TdvxebQhnxfMWLYqBo6VhChKxcTPNJyXfUa2mmnpxzEf4DHGpyS9qsXf90h606TP5DiO45xSPBPTcRynonS2mNUR2dOHiWCYzkbYnI6tLMnEZCgUsxVrsDn1q9M5o/ANDzhWUgCIGaiFhSwAfWePcnoPV7KpYyE7Fu00hqn9Qmyn3HMu7Dps9ksqoXBfhnLWYDM0i+02BLsPNsPMKGUtTT4zvM0Kd6MUYPYx2pWxW2moKDNO6QvhUC2EV8KmtFmHzQVHsmOhWNWIkU3NMEO2E+9DS0ZLxy/vEx5rG2SLHgyIHkgmbMdNuFYuPpHeG4Vz82AcjBPjv8Adx3Eqij/AHcdxKoo/wB3HcSpKZzXwLo1rj9TCqdWyah5hyNGakn0L2hhshn3xWFbqM0VPS7fenuhjVvgbQ7Ho+1rYQ7AZHkXdMdXjasa5GUZYKFEAm7rjWtjUyFPf6tjG/uaxqYFTC6buaBXg58LVKexfhhVSrx+CzXbrg81QPUqin0/GG+8TnpvhbDwXr7vsPpKK4y/tlyGey1hImGF/vM8oDfMdy/thszTDN5PP7O8a7EVoZJ6rDzbb4aCx4HJ6fvbZflbC1KTxX+CO4zgVxR/gjuM4FaWzEsqoxrPI1mEbp9P3w+Y09XWweSXpFIjTSE5vOGWtGedm5bs+2HXYlFzS4zPpitNGTrcpDfwC7CHYzPIrTN+Sz5ZURNvavwZ7KWxKNPXkM7Nbz4JtZeZS5qA0MASblewoLfxT8pnXwWMxHI6heH2w18JmH3OM/GbJvsSSkthOddgc+6wa2pd85tKePPeQyukzbGsdSo71ehvnXgf7y7CZuckURla/5L2RSk3cl33QSr67p8Xf5L/AHcdxKos/wB3HcSpKZyWUoPGoiLJiQZL0Fth9sDlF4RqKjyef12KbtZ7iZtiUWFiM6N/A5pR3I+xUxigUB4JN6YA9xoUI3gv7TNgvwk4Lz/O6L4PNKSvbnJEibAdKD5RB0n6xIjXqsDnVZzs9D5vjh+Pxcdjp9Jy+cYEPRo2wTzkd5higzXZMoy1+hm2MQqnDZnErSipc/5UREffCTqUESmiUoQgjiSgtMLLoCtjscy6ikPYxj1WDPQR7LWxKKJTJ2Gd8pqTj8dvY9vuGLyX4L3DHcZyK4g9wx3GciuIPcMdxnIrSWQ18QNLvND9T290Cm6FX1LuYwfiLsKkNp5Rpr63ONQ+C1rq82Ra+IxfvRup5fNzoZlQy+0by+Vqci5mTVjYjfXsBXcprpeaZanvMhmXoHXVkhnWxD2uw2adloZ/U26kxUpPk9nko4fclrHTA8UddmosHpGOEGjd1Y7Y53xVcBfvdsJnRSh06DYu1QjOp7VuL+74P4+kJdFId+7NPU3hf8btW2CHf3/Ce5rWcAd9/lPjOsGO+V6jBZh+wMiJZg5cL2/IXQgtfM/6M2PMr6DTrPpKkz7Q+rf8CdxzHqSj+AHccx6koHZVQwqIjmvUvh1tuG8Uac7NvyPeb2ZVrCQdf6MvsM5flcULzZ4/PwY9Al6A9H3rMYu3O7D7MYR9esTqzFzGmCHVtNq3JU8RGNX6ts9fk17m4Pz/39ofyeLfLVv8gs2dCYxlaUcvsI0fza+2bUc/sXavGY9L2z8t1iCVX5isR1If7Mnv2Gfm08cUBzsdzFq7JpaaFM/J2qyexgrufzSsTnX/2o5m9bVkey3fo4OzMXj3wcGZvPvSa3BmGTDL8sga7sOZqgrXAwzrYa3kuTL957iOIz0ybmX4xg5XSEjNO4cu5K/J23rUo74f923HAvuQznyZWATEMl9m1/F7o669n9u6HV2T2mbX84gdm5/fOYzsvOf558eXP5Md6ID9WQXpalssxl614ILO3IZb00Gg+/rrPHs2/n+hoj11+QbatfrQvs/ecQV1sYvwXuOM4TkXxB7jjOE5F8Qe44zhORQkxcsXU03iyZYNRv9PUgp7GRqYAM12ZoVg12EzjTSt6DWGbteAC06yp3bFwPEP9GILEUK5U8uS5+F2m7LJqI9vJgrrkj5LP1AGpn3JxXobifRk2pbya8X36lsI+ux82dWeeiyGSVhjiBbDTEDYeq0/l8LqY8s12Z7VLjoH0tUYN2x6AzbHKRRT+BWy249/DZkp6Ks/3YRvD/ngfMZSPIbTvgV2HzWcGQz/Tyon0je2wETbDexmyyHcN7COG4JYtlMF2+YcW+9wVNsQY+QbDf4E7juNUFX+AO47jVJRJhxGGEGaqIWTsiDG+I4SwUtIX1JjwbZD0/hjjaNkxNKZxqeP12FY3HFgHm9NzVjJLpyUMGbsBNrPwKGtw+v4ayE7PIcvv9fjCf0Izp+ev4dj/ATav83OwOdVbB5vTM2ZTpvLPELaxsDylIO7/AdisHsepIWWMdJrKhQOYtfd/YFthgKRszUup6Fs6PedEdgg2r5vHsiQ6jteyCoKWhMap/b+CfSlsZn2eB5uySNoWHB+8zo2wKQdyQYb7Yf+xcTw+zdIMR1aM5LkpcTAi9huw+fxiO7HP0+qWvE62ubVYSUI7v8A/pDzJ86OSPhZjXNV04eY2juU4juOcJJN6gIcQlqlRbeSTTTtIukbSnc1dblfxd63jOI5zGpmshPJnkv5A4+/LF0qqxxiPaQXbVZzsS5JCCLdIukWStGjF+JSLxWEoU2Ca2rMsn2cc3D8n32EI2WrpNJTTHV41pryv/K08zGDn3rMze+xw/rp81ZX5K+0a5tTf+sA78xP0jUswl53zw2zThg+g0hGnoe+HvRE2p+OcjjHyIy2MhLf6r73ue5n9k4VvzHdA9M3iK/Nstz7oYgdenffZyGgentGVZNvu+XI+nC56Y/7af8tf53P/eVfkYUj7H0FYSS03i0WdoFN8H+MpLdrEiKgbeKzcnLEw1x26e3KlsTCW6zg3JZn6xOcqQKnIijTieLGiVNJiakO4sXhPW4u4WHA9V0aesfjVv04+b8Q2Xjcji3gutgufKYyoYb/Uks9sQ47FVjAaqIn5CzyE8A5Ju2OMVJEmRYzxthjjYIxxUL18ejiO4zgnymR+gV8l6Z0hhOvU+LejV9LHJfWFELqav8KXqfga0XEcxzmNmL/AY4wfiTEuizHW1HjX/50Y469Luk/jlYxvkvSV0+al4ziOU6CtTMwQwjpJv98MIzxHjTDCfjXykt4XY2TAU8bcwQvjRes/JUmarVwHHFVezatHBzJ7JXTl+3R1Zs/B/pcnKWnPKtewz4UYtweC2JXKdekBpJ89oMszezG2X4IYuLuVa+Bp1b3rdXe2bSPiuu5HXGAPVsjdsDdfeXh5f57KyXamLp3C6/w1/U1mP4nYK1ZkY5/tRLsvR5opv/+GZLXoz+o3sm2/pLsy+x6Iu4sgiG7VysyuIxWPvqzCmBiBWHxA4zp1N+L62Cesdsl3IhzrT6Bd63hBxLGd+saxXUO67EHl+vpCxBXSV1732Xq29PspBworE+RwPJBdSBulbxzLg3gZcQjtmlW3xLFZxXMTYvvOLaRq5tA3wna7OHkmcNzzWLwOSfpi+OctMzHbKicbY7xfzejMGONTkl7bzvcdx3GcU4dnYjqO41SUzi7ooFiYBh2DU5oFmOpz6r9aecF+TqHT4y3Eti6ci1NUTmm2IgaN00gu6MApEo+/J6lWNISpPuHiErzOlf35lJnty3bl4hTdyf68bsoOBzAdZ59wYYyzUbWJU+yL0IfpFPpiPZRtm11IRyyHchCPR0mO11Kc1o6H9vE6OD3nFNhaQORiSG5zIMmQVIKx+ohjcSZi+9hOHD8cu5QW0vNbcswAxvIQ7isee7cWZzbvBbYjx0gqZf5QV2LfUdj5dyllrkX1qiN4dLIdKauyLcpgO5Xhv8Adx3Eqij/AHcdxKoo/wB3HcSpKRxd0uGiwJ352fSM3nCFFh5SnD1PjpnY3iv2pYaU69z7ohNTAqa1Ry1t4KNedj3Tl+lfv7lxPe6k//3dxY3eup1FXTKFmTb2dIWzcn5poQacezvXVmUmzHsEbkZlIhQ4vwlkuZMF0Y27HYs9MKY5Jsz/bn69qcBC6M0P3OD7IK3bC+WexA9OwESUWk+0B1/VSb/nvoLmPHMUfsAMy52W1cyINH4LfYzPLfek6kvvSjeqCEdcd+OoBUYQxuY1H0Z8H5uR9Rn2e98GcA3mf8lrm7kY7Wm/wkvG4b2Xuyx6smsFnBEOLqXHzWrh91fN5bv0jZ71yQje7oce30stfFR7zBR0cx3FeTvgD3HEcp6J0VEIZXBPi+mYB2jEkZXH63v2UcTBOn2iXzag5feaU1YL7D8OmVMBi7ymMGOKUlbIEz83pONdf5PScBfnT7XuxjUl39I2yA33htXHxAVZlmzvBZ6nYn/S11zg3bbYDz1cmqXCsWeOH4419xKQ+SiYkvVb6bckK1vjidbPdOdbLxra19qwVLUff2Gf0lWuJpt9nO3Ffqw+4nX3IMfA47LS6If22pEhJ4Qq5hOI4jvNywh/gjuM4FaWjmZgHzjhDD17YyDy03kgvWZPPO+ccyd9Qz3kpf3N7YG6e/bZ75virekZqzL6wfOlORjTw3L3P4vuczkNCGcOU6VD3+L+bB7vzdmAUCaNQFuxElp41nefUj5EfiYRSiDgg1pSYsF2s6XpyLc+tyjcyimTfkjyqgNmPhT57HH1mSVH0LWk3RmpY0RcLtqLP2M6UBthOlJoWT7xtuB8FnWb2ZTajtQb25g1RiDSyfE3HH+S6mCdSFqBs2mVEvBTgdrZT2o4r8k2HIKEw4mWkO48EYVEwFm5bcCBvuG7KqCl54q7G0G5slzL8F7jjOE5F8Qe44zhORfEHuOM4TkXpaBjh8sHF8XfXv0dSC50ZmXUsiF7IjjQK06cVBHksFsEn3J++sboht9MX6v3pdmZdMUPsBSMjjJomq+yNwRdWUUt9oY7Mam/FQvQTH0sqtiOPR9Js3Ge1JNvGio/sQy4GsFz5AsvMvrUWumC/pOOVmXPsM8IsP8KsYx6f/ZLqscxg5jsUq1oh78MVWOiibAGQxvfH25VjkQtTsJ2s/Vl5cx7GD8cE33Wl9gji/qyxvAuVEJn5zWcAq6cyuzvNoOY9yetmtrUkvTps8TBCx3GclxP+AHccx6koHZVQLrs0xAe+2/hcCBki+2EzRIghbSVZgocQIsYiTaTLykBkmA+OzwJDW+fkhWzSKbIVjsTwJhYjIlYxI4YtcjqXsmi4vRTVF3rzC6cUsGBz+UIFKcMX5bJB7978WAzle7Q/b+MLdz6d72+Eag31LstsSiipVEEpiFNeynuzD+XbH+u+ILMvfX5L7iuubVd/3q77E98ooVASmY8bqSjv5SGPXEygb2/eZ/Ql7WNeJ0PxGNo5OjPvY15L3yEsPnIoH/uFZwjDL5OmGF6Rn6v7UD6eds3JJZOCZHsk71OGMReeGWXZlUb2dKviaPPOOOoSiuM4zssJf4A7juNUFH+AO47jVJTOViP8hRDX39E0qF9ZRe2taoNlVfj4XaYDW+nnhMdDVGKExsUU9VSPpab4ikfgDFO66Ttty3f4li5UUO8vT08vLIpg9Qm3P2b4luqGXOvZGC+FNs/Xei76gu+zOma9Nx9QqT7Lsg/Ufke784Mv2JZrvy8sz4+9aDNe+HA8MkoxsZk6PzIz152pI/MdSiHtn6UaWDGwbFEOXEbk2AWBY9d6JtAXasn8fuqr0aZ8R8LyCN15VGrxvqNvvBfoa0pJSYljhLVejdBxHOdlhT/AHcdxKoopoYQQzpD0PTUmNF2S7owx/mEIYaWkL6gxGdkg6f0xxtIyf+cPzo+fWL9WUjEzjjADjOFuDOVitbB0ysv1FIvZaLkvVgYh92coFn15AfO1NPSKMgUzDJkBRt/YTpzeMzSL7E4yznhuHtvKTuO1MIzwYm3KbGazpVmD21g+DjCzjtmw7DNm/RFm4jETtCz0k+snjqId2I78/io9mdnWOqepzTbnvgyHZDtxvLDdFiPzl2M7vZfYn+xDnou+Wuc6F+1UrNyZnz/tl+J9kbfbEDQ7jhdmWrKdCqGj+H56r7B/maHK55MkvT3cf8ISyiFJ18QYXyVpraRrQwhXSPqopI/FGFdJ2ifp5kkcy3EcxzlFmA/w2ODY64lZzf+ipGskNRdI0+2SbjgdDjqO4zitmVTp8BDCTDVkklWSPiHpSUn1GOOxd63bJS2d4Lu3SLpFkuasWKi/1G9KKk4TLtCjmc3CNZRcOIWhVJBOmShh7IRUwO2curEwDWUMTpkJrzX9PqeBD+nizOYUmOdmQR9Oz9lunM6n186iOsxGe0LnZvZuFJDisdmnjyrPQFyOwkmpnMPrZn8/rNWZzf5nMStCyYQMJcXQJOlcPTHhdykt7YfvhGP7EkhLLLRlFagqYxPGU/1o/t3lM/I+oEzB8VZYOCORZFhobSvakGORfUZZjGObfc7xyXZLpU2ONUIJhbIoxx8zXK1CbWm2LiUUFmJr/Ty5v+VxJ/USM8Z4JMa4VtIySa9VvkSn9d3bYoyDMcbBM85irJ/jOI5zorQVhRJjrEu6T9LrJPWFEI79FFkmacepdc1xHMcpw3yAhxDOCiH0NT/3SHqLpC1qPMjf3dztJklfOU0+Oo7jOC2YjAa+RNLtTR18hqQvxhi/FkJ4WNIXQgh/LOlBSZ+yDvRznXFcB001RamoebIIPkPSGBZEehJt7mFdlG2jdkut7nI9UHou2vTdCuWiBpby/QNvzM81Jz8X9dWHoQNeqo2ZzcL11ALT7WXvESRp0yj0+X25LysHhjL7e3pDZlMnpOaZaqTUy9knDHmsYSGL7wvtiGt58pn8+MtW5N/fM4xUvd7xj3ynwet6dDg/9uLeXBvuQSjfE1qV2awoyHcTP3r2yuOfZ6C05uqBfHGIJ+/+hcxe+I58oswFQ9iuazGeyt7JMPSSYYQMl2Of8b3DtuF87F7em48B6vMMNU59ZahnWQirVNTAH8N45Lm4kApJfWWbs12sRTRSzAd4jPEhSZe2+PtTaujhjuM4zhTgmZiO4zgVZVJhhKeK2Ro9Pm1iKFaxgD7XZ8zDdjjtoCyRZkZt0iXZNk6nGTZIiYVTWPrK6RZhuFM6VWQI48I5+dRu93Duy8ze8gxWwnbh+VIJheGSlFs2zc4llNEert2YSzBc69EKtdqahHJx3wcxCaSMYYXWbdubT8fPfEXezgePopDXQWTI9o5LCwdG831H9p2f2ZQxOH1/EuGYDN2zMhZ153g7H12Xt/mTPbkcQ/ZszqN9l1ySV/2y1kEdGq5l9khS9KuQgTyKDOTZ+XihZEKu7P1hZlNWo68cE6nNEEbek5YMynMxXJfw+2mI426EnfK+oW9l+C9wx3GciuIPcMdxnIriD3DHcZyK0lENfKaOHNeSqBszDZahNJZ+ynTUdH+GDLEiGzWnN+j7Bb/z7x8o3U6Ni2GKqSZ6NVJkv4/Qu+W9uQ7N0KmN0IapzTG9mXpbGibGlHD2wflIjd/Tm2u7DPVju1rhUWdr5/HPX9d12bZf0R2Zze1XinpprkuP9JdrnDuP5uFzgwMbMjvVgvtm17NtywfyPuK7A2rDbCerXTj2t1w73ufnn/9Qto068PaB/P3NvFX5vbDzmVzjPn9Frt/TN4ZEposgs9TC8tl5O1BnXqsHM/uHuiqzeS3P7srDFN828M3Mpl6fpqTz/Q7HJss+MNSP6e2859lOHF+LkmeQ9Y6Dz4sy/Be44zhORfEHuOM4TkXpqISSZmIyDJBTGBZIt2QMTnFSmyFElgTC6TcrlVGSYUgkj0/ZIz3encerETQoVN07kFdge2xOHkpFmKnJjLGyzMx/+J9vybYNfjCXERheSb569PrMXjwjn7ZSzqGUkPq252g+PjbOWDvhvlKxHRehjzY+k9fCv2hFXgGwNmMosymDpBmKTz+bj4fLzl5f+t0fjl6Z2Q/Mvjyz36ZcCmBmJq912fnjWcy8T549gMU9l+cLC+z/cS4NqJYv6LJrNJfRRmbn44n3aXofs13WnX1fZm9+7DWZvXNR7uvy/rzdfvRMLidqfX6fPfzL+b3xwK6J8wp7BsozmimxsPLh9qdyeWjxObmvlFCG9tYy+0D/uBzEEOktR/Nz7flGy8KuLfFf4I7jOBXFH+CO4zgVpaMSSo8OHs/O4xSXUSRWQXVr3cC0UA4L9PDt9qMHclni3XPuzGyuWcgCPyzi80PlU+ay9Ty5TiQLGc2ck18np7DMZuO5rDU1s++vyTYVfPuzx27N7Leef3dmv3nGPZlNKYG+sUDZPXrz8c9LZuR+Mkpp9/fyQkk3v/ET+XZE1Gzcf0Vml0lureyR0aTw1sa8TWeezSy//LYamJ1Pz9mu5P696zJ77HDebkeH5h7/vOhyyHlj+b5rzs6jVOpn51Ep2x/K5ZrRn+fX9uIjr8jssy/J+2XraG3c2Jx/d+HZubyjrlyuoWTCe/z6FXdl9sMrcqlhpfIs0iMD+bWnzwQem/c0t/dpX2ZfcM5nMpv3HSWUPf0TF6jis2vRjLyddl1XlCp/VPhLA/8F7jiOU1H8Ae44jlNR/AHuOI5TUTqqgR9S93HtiJoTQ8x+AB2ZCxUwo5ChVpdpQ7Jvub65ek6efcbqcNTXqXk/ibAvZgV+Xb+Y2WlG2puV68ZchJYhicwgYzsMIBzqDv1KZr9fn83stN1+7ZpPZ9vYRzeef3tm87qZLVlst5rKSN8tHMQ7EOrG3W/Mw+OuVh6yxvcQ/2z15zK7Bv10g/IwQ/bhxbPHzz9yXR6CxncsfE/B8cJ2egBl9ef3YcFu6NLXXT7+joZ6+/m9+bl53zBEceSSH2R2YRHsS+qZzYqAb5s9HgI5+tbcT95nF52T+/IbGIv36erMZh9wAQiOL2rL1+nrxz+niwpLxfBL3mfFMGYuspGfm/vzPVu63VrQvVXWuWvgjuM4LzP8Ae44jlNROiqhBEXNbk5lmIm5GBLKIkxJOPVjONwS2OmU6M26N9vGqR0LQHG6xCI8lDHWYfrOBSAYMllWsJ3hlJQOGB7HUCpO5a7XVzOboX3pdJ59wmniYCJLScUpLhcqoLTAYkecKrLIWApDEK11JQnbkX3C8cbpejqeKFsxpIzfpWTCdqbs9eEZH8vsLoSSpn3MPqLNsc3sV4brsvAWxxOvLZUnFyCUjn1SvM/y6xpUntHK8cEFQji+blAedpjKS+wjyhjW2KS0RDj2mdmZFoqj7LUK6wPTtzL8F7jjOE5F8Qe44zhORfEHuOM4TkUJMUZ7r1PEOYML4n9dv05SUVe2oLZHTZO6cqr1MsTQWhyCoXysTMZzU48vhhTlOmMafkftnzoi9bDZCIeyUuW5iDH11rRdmQ6cprZLRf2cMDSPPIYQNJ4vvTYuBEy91HqPwe+zXTgGOB7Zh+n+PPfdemdmsx14LpYQ4BiwFhxJ+6yd4v+StBO6MduNKeZWu6fXxnIH39TbMpuhnnzfwz64R2/K7A/qf6kMhiKn9w7vYd5XDDOkb+0sNCwVx1vazrxOvn/huSXp5vA3G2KMg/y7/wJ3HMepKP4AdxzHqSimhBJCWC7ps5IGJEVJt8UYPx5C6Jd0h6SapCFJN8YY9010HEm6eHB2vGt9Y3rAjC5OK85FaE2hmtjw06V+dyWzkn0rsf7moXy6dLA7n04t2ra/9NiIMpSGYeeRfoq5yqE9/eOV7ThFpW+7u/PpFPdnuyx4HqF4P4dvh2Cnvr2Ybxq+CBX5ZjIDMc8wHNy7ObMDjvfCynkqI53WrtiZSz2PLHllZq88kPf/tjnLMpvTUE6vFx/Jjz//xXz6H16Cc7iWlH1r8vHF6Xnfofy2mLv3aGYPL87buXdr7kuhD9NL41A9IzcPnZnbu+bkg5PSJGWthYfycMvuQ7nv6X3GsfXSyvz34dzh/LtjuC/u6V2X2eyz1Qe2qIzuZyfeFvtzu95f/kyod+ehwFaILDM5GWLL+zZl/vDE4bPHmHWmTlhCGZP0ezHG1ZKukPTbIYTVkm6VdG+M8TxJ9zZtx3Ecp0OYD/AY484Y40+bn0ckbZG0VNK7JB0rjnG7pBtOk4+O4zhOC9qKQgkh1CR9T43S/8/EGPuafw+S9h2z8Z1bJN0iSSteocue/nJzQx4coTHIDl2cDjFn9HHYmI4pnQKvwDZOSTld5kxtr+ELp7F8iYyFErLZ1w+wbTlsTIELU/m5sOkrJRPKP+nx2S7sg/IlMYt9gHr+OsfYP/VtN7bxu+wjTi7ZTrTZLiUSiaT8WtgnGMuFduP4Wgmb5+b44vdTOcD6brvgPiycuzwhMYeBG5B3Cu3Esc+xzGtlP/Da0/15n5zX5rEJt/Pe4fMpvXYsW1o4N9tcUrjuxCWUxgFCmCfpbyV9OMaYqb6x8a9Ay38JYoy3xRgHY4yDZ/VN9myO4ziOxaQe4CGEWWo8vD8XY/y75p93hRCWNLcvUfE3k+M4jnMaMR/gTXnkU5K2xBj/NNl0t6Sbmp9vkvSVU++e4ziOMxGTUcyukvR+SZtCCBubf/t3kv5E0hdDCDdLelrSjeaRhiV9q/kZ+mcXPaFWS/2MXAY70fIeWZOHoF34AEIQ8wJtRW32KdjUAalZcS7yntzcsWRcNF36XQjF38rNgnZXFgYoFbU52tD+xtaOf+5iIiWv6/+qHGrBbIfNsKm3pn1MnfgZ2Hx3cDnsvyo5tlRsN26/EHbajhdj270qh336S8Z2XgvHQPpOh33G9w5sN4a8Urtln/C+vAh20i6PvB332Vdxn/FcP4bNtZ6Z+ItQwMK9wHZLt7M/qUOzndiu1Ov5fLLeWyTjbez38k0jvXlI44Lv2GGFE522QIzxHySFCTa/aYK/O47jOKcZz8R0HMepKB0tZjXYHeL6VzQN/va3asVQtuD+PF46RbZCoTgd4vS9JMNLUnEqR8pCkhBiePjruT3LCoHktXCa2SIkaaLthxHONAtT9x1ohx60eQ9kiWHj3D2QLQ4m1zYAOeYw+ugg2rwXIWgH6StlCEIJpWxMcGrP61ybmzsgsSzFdiSwqp99yATWdDulIMpW1tjkdVpQJqNkU8JBjC/2IWGfD0D2GEIf96MPx5Lv9zNs0LoveM/yOi2JF9uHk5DIXspU7MMWoZrhqZMMI3Qcx3GmF/4AdxzHqSj+AHccx6koHV3UWHM0nvJclvreyraq6pVp4FYKOPUsapDcTt8sfbVMv4ffs6jVUS8jTDfmuwErlT5pp1kMrYL+vpT6p9GHPWwnQ2/tTdsCx6YeP8vQ/guaNzVKazzRTrVlK3QT25cy7BD7F/RZS7cu2271EduB7USt1xo/ZWOf70jgW09ZWQep2Ef4fo33PPdPse5xXgffO/C9GNvFKM/Rm7Y7+4S08VT2X+CO4zgVxR/gjuM4FaWzEkqfpOubnzmV49SN0oA1teO0JJ0ScTrEfWkzC4u+WpXrOB0rm2IzTPAq41i8blY9I/S1bJrJ66Tf9I2jh9lsbYSYScqvlce2pAFutypOtlutMIV9QptSFLGq7LUzvsqkQ6nYLlamJcMQd8Eukw6sUF/rPmS7WTIFqxcyyzSFfWRJKFbFSfrCc/NeKKuMOBkmWMvCf4E7juNUFH+AO47jVJTOSihnSvGdjY8H5mK9vK35enmFKYNVJL9sCmRMIw+hQE/3/zDOZRWy4TSVUQbp1BHTyvve/TocKp/TrjrwZH6ob+LY9JXT9ZIiO4VpI6/zl2Fzqv9d2FYBIJK2G+UcTpcpPXGBEBZLY+Ek+rYTNn1NbfYvp/7wfd/bUazoU0axIivzNx3rViQHxt5LV+G+exb3Hc/9CGz2eXp+S9Zku7EPGa1DmWK9cTyO9dQ3jid+l+2G7WnRN0nq4rU+BpsF8tL9T0RCmQD/Be44jlNR/AHuOI5TUfwB7jiOU1E6q4HvkkJTX547DO2NWp4VVshwpzJ9lVeJEKLutdjOBXTfCvuTsKnXs8A/tb5UA4PmePWDP8r/wIrrd8FmmJdVsJ8aeNo21OYs7b9Mc5SkjbCpGzLUKv0+3xtQm30bbFxnF7/PBSCs0D2GxKVtQ22X/YtjLXgRmjdD2P4CttVnKdRyaaPN5+7GfUddmfeV9e5pbILPUjEDmuPrV2H/b+NcvM+sBZ3T7byn2Qd8V3A93hX8NdqNi7ywQmXZewxLA7cWVE7wX+CO4zgVxR/gjuM4FaWzCzrMD3E9166ciHZD9zgtKZNUOD22Fkkgli9WpmcaOshz81i098Nud6GLMtGMflpTVGsdQF6btWhC2ofWwgGW3NMuVp+mvluLj9AXXrfVh6SsT3lsq/+tUE4r+5GkY7mkUJoku/+tPiDW+EyxwgYJxxfPxT622i31zSra1eI6wud9QQfHcZyXFf4AdxzHqSj+AHccx6konQ0jjBrXiizd2dI0qa+V6dCWVkv9qixduBVWFb8y39gOPJelvVmLq5ZVaaTN7zJM0FpowKqiZy3CURZeRU2cWq8VBmhpudy/bHzRT+vdQbvVBknZ4hRWZUQrJNFarIR9Vqb/sz95H/C71rmt+64sJJa2tTAFx367z4h22q2d9y0G/gvccRynovgD3HEcp6KYEkoI4dOS3iFpd4xxTfNv/ZLukFSTNCTpxhjjPutYRw5Iwxsbn/diancY+44Yx2Ii1QCmjj1p5hWnuMg224WMwCewO2vHsdF6YXOtvv6yECZmcCHEbAd8ZaIlfZsPe6HlCyoxZmCxiIPos2FMG5kUOgt2D+xe+pauO8gp6BrYzHRDOx5E2OFe+MrxRrvQbmm25YXYmeML7TaMThtCO7JdyACm1L3p+a3wOLYTfDsMKWA3bLYL+3Rxcr5Z9MVah5QyBHzdCwnvMKQHjn2246ykD/u5pqVVUZLtxhuPvuJaDpbIP4W1ZY2s0DIm8wv8LyVdi7/dKuneGON5aiSO3zr5UzqO4zinAvMBHmP8noqvs94l6fbm59sl3XBq3XIcx3EsJpWJGUKoSfpaIqHUY4x9zc9B0r5jdovv3iLpFkkakC772+bfLe2GEkq7MkY69eO0j9NCwnOVLbU3Gegb7bJzk3YT66x2Sqed3Jd9YE2nrXa1SGeSZW0kFfuEvlhSkwXPXzZe2QfW+KGv1r1QkJ7aOBZ9s/qo3XZL61VZ44PwV6E1/rid7UJ7rGRbP+QdSroWluTLduuf4HMr2C6StFqnKRMzNv4FmPBfgRjjbTHGwRjjYN/JnsxxHMc5zok+wHeFEJZIUvP/LELpOI7jnGZO9AF+t6Sbmp9vkvSVU+OO4ziOM1lMDTyE8HlJ6yQtUiNS7A8lfVnSF9UoZf+0GmGEraSbjMHZIa4/q/H5sJFpybAc0o9wqVllCxczhIghQlthw7cdrIQHeiDOdcHuZYhSWpGRoVYbYcNXhlKNwea5ZzEzj76k4XEU54wQtLaz9qzMy9Q3a4HbzYYvjGm0qjZicWktKdnOMEK2C+ajh7F9FtqBfVroM4YGpu3EsU3Rm2Obc2Ujw7DgW5kv1ljjAh70DX28C772MlTYWqg4bWdWQbUWiOHYZ9gz2u0g7FkYXz3pywL2mZUtLSl8qbUGbsaBxxi5bsYxuFaM4ziO00E8E9NxHKeidLaYVZeOFyWaxakepricHnFKUpjKcfp0+fjHlz6K9e3+G9a3+zq+C8mk3wgx6ikrNiQVp0z/dvzj1svyufrKX9uZ7wtpYBZ8KYRqWQWF6Eu6tiQnaN+FTVmL005rwQdOFSnZpL69zzi35YtVkIx9RF+4huLy8Y/7/kselLbgPyNoDGs3zuJ0nDIXx5c1ttN2eg+2URrg2G6zSNwsq89S37g26DWwuZ6rsaYqpSbKg20Vanu/4Yu1Jm+bYYYFX1NfKC2xfwtCiaQvtT6P/wJ3HMepKP4AdxzHqSj+AHccx6konV3UeE6I61c1DVYWQ4jR49DqrFRVkkp1S7FtgNW+qHcyhOw7ubkDvm7H7vSVElct0V97WWWPOiJDr6Cn7oDN6DnClOKlqS/LsfG68nMXwsBgD0FHtNK0Uz2fUmuNYaLsQ+qt1O8RyjcEzZxyLFOl0/T1VQxhZR++DbbxjmXH46WbC/Js2oc1htZx/PCdB0GfDcN+wqgamvYhx9YA7KVsN2rB2H4Y7xJ24L3GDny9bHxxPBXGFys+st3QzgfhG59XfCWT9ukcbGO7vVJFzj9dqfSO4zjO1OAPcMdxnIrSUQnl0hkhfrcZ+sMMQhZAL5uqtdpehlWxjVMaTsUsXyw5p1BovuTYnNpZ1eSsc1sV4coWE+C5rIUH6Ivle9nSgNa5rO1WpcR2q/Slxyvrz1bH4v7W+GnHF6tqoxUnzHY4ANuqCDhWss3qI/puVdakb+3ch1afWdtJu8+EsuqohIuySC6hOI7jvOzwB7jjOE5F6Wgm5swuqbepEQwb691Rxmh3ytszwWepuH5mIWsKb7sZFdCuhFI2XaNvXIuR7cJoCWvRhbJID6lctuAUdz7e1HONTGthA0tiKZtyW+3EPqREt8eQ6CypoGwb1/acD7sHGYNcd/Jk2slasKNQ9A1f4LqmB4yokzIpypJQKFUOILKIY32EvrGQG45Xdh9y3NMurM/KiBjeh+jDdmTXMumwlW9l+C9wx3GciuIPcMdxnIriD3DHcZyK0vlqhM20xL3I6tuEXS3d2dJP03A8rg3QwwwwaOIHNuY2kq4KWVbWoscMDUwz1FZhG7U3am27cOFP4PuWnsoOT7NUl2HbRVyYAimlY8wKhW/MUDWKz2V9WMhepY0+44IeO57KbbYTfbFWI0lDu5hh2IsL6WeHox33oE8t3zi+Up27kFEIe4AV+zD2x7AwxtPYfQh2WXgc9XdmQBdsOD8L71j24KHAdrIyMVPYZ4XMTA5GZCXPQqcwK5RrerAP03ajxs2wQSucMsV/gTuO41QUf4A7juNUlI5mYi4PIf5u83O7WXrtUhZqRbnFOpeVKWd9n+crC91rN2vvZNuxLKTRajfLl5PJnrXaxcqubTekkb6VhXJZ7UIpod1zW76XjR+rXU71+Co7tzWeTnW2YxntZj9a2bXkVPrWKpz2g56J6TiO8/LCH+CO4zgVxR/gjuM4FaWjYYTzJF01wTYrvdTScssupJ3U1FZY+tfJpMZa+1paGr/fjvYmlaevW6GaJ9suZfpquxX/SLvVCNvBumnafVdgHa+sT6m3W9dp9eHJvntq51yk3T629Pl27vtT/SAs883yqx1f/Be44zhORfEHuOM4TkU5qZlDCOFaSR+XNFPSJ2OMf1K2/0zZBeiPYRWDP5npUjsLC7SiXUmm3alhGdZiFO0eu+xaWEB/MuFOZbSzIES7i0eQdkPUTma6XZYp2erYZetttqJsOs4qelwYxfLNqjbI8UXS07NPrHDddiqKSqe3T0/mnpzMudqRUNrhhH+BhxBmSvqEpLdLWi3pV0MIq0+VY47jOE45JyOhvFbSEzHGp2KMo5K+IOldp8Ytx3Ecx+JkJJSlkrYl9nZJl3OnEMItkm5pmodWS5u5zzRhkaQXptqJCXDfTozp6tup8+tUho00mK5tJv3/7dsrW/3xtIcRxhhvk3SbJIUQ1rdKB50OuG8nhvvWPtPVL8l9O1GmyreTkVB2KC+6uEzFCo+O4zjOaeJkHuD/KOm8EMLKEMJsSe+VdPepcctxHMexOGEJJcY4FkL4gKRvqhEh+OkY48+Mr912oufrAO7bieG+tc909Uty306UKfGto+VkHcdxnFOHZ2I6juNUFH+AO47jVJSOPMBDCNeGEB4NITwRQri1E+c0/Pl0CGF3CGFz8rf+EMK3QwiPN/+/YAr8Wh5CuC+E8HAI4WchhA9NI9/OCCH8JITwT03f/qj595UhhAeafXtH84X2lBBCmBlCeDCE8LXp5FsIYSiEsCmEsDGEsL75tynv06YffSGEO0MIj4QQtoQQXjcdfAshXNBsr2P/DYcQPjwdfGv697vN+2BzCOHzzfuj4+PttD/Ap2nK/V9KuhZ/u1XSvTHG8yTd27Q7zZik34sxrpZ0haTfbrbVdPDtkKRrYoyvkrRW0rUhhCskfVTSx2KMqyTtk3TzFPh2jA9J2pLY08m3q2OMa5NY4enQp1KjltE3YowXSnqVGu035b7FGB9tttdaSZepUZrnrungWwhhqaQPShqMMa5RI4jjvZqK8RZjPK3/SXqdpG8m9kckfeR0n3cSftUkbU7sRyUtaX5eIunRaeDjVyS9Zbr5pkaNo5+qkXn7gqSuVn3dYZ+WqXFDXyPpa5LCNPJtSNIi/G3K+1TSmZK2qhnMMJ18gz9vlfSD6eKbxrPQ+9WI5PuapLdNxXjrhITSKuV+aQfO2y4DMcadzc/PSRqYSmdCCDVJl0p6QNPEt6ZEsVHSbknflvSkpHqM8VhC91T27Z9J+gNJR5v2Qk0f36Kkb4UQNjRLS0jTo09XSnpe0mea0tMnQwhzp4lvKe+V9Pnm5yn3Lca4Q9J/l/SMpJ2SXpS0QVMw3vwlZgti45/QKYuvDCHMk/S3kj4cY8wqgk6lbzHGI7ExpV2mRjGzC6fCDxJCeIek3THGDVPtywS8Psb4ajVkxN8OIbwx3TiFfdol6dWS/jzGeKmklwRJYhrcC7MlvVPSl7htqnxr6u7vUuMfwLMlzVVRku0InXiAVyXlflcIYYkkNf+/eyqcCCHMUuPh/bkY499NJ9+OEWOsS7pPjWliXwjhWELYVPXtVZLeGUIYUqMq5jVqaLvTwbdjv9gUY9ytho77Wk2PPt0uaXuM8YGmfacaD/Tp4Nsx3i7ppzHGXU17Ovj2ZklbY4zPxxgPS/o7NcZgx8dbJx7gVUm5v1vSTc3PN6mhP3eUEEKQ9ClJW2KMfzrNfDsrhNDX/Nyjhja/RY0H+bun0rcY40dijMtijDU1xtd3Yoy/Ph18CyHMDSHMP/ZZDT13s6ZBn8YYn5O0LYRwQfNPb5L08HTwLeFXNS6fSNPDt2ckXRFCmNO8Z4+1W+fHW4dE/+skPaaGZvrvO/3SoYU/n1dDuzqsxq+Qm9XQTO+V9LikeyT1T4Ffr1djSviQpI3N/66bJr5dIunBpm+bJf3H5t/PkfQTSU+oMc3tnuK+XSfpa9PFt6YP/9T872fHxv906NOmH2slrW/265clLZhGvs2VtEfSmcnfpotvfyTpkea98FeSuqdivHkqveM4TkXxl5iO4zgVxR/gjuM4FcUf4I7jOBXFH+CO4zgVxR/gjuM4FcUf4I7jOBXFH+CO4zgV5f8BebgyO1vpaXQAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dir_HC = '../wav'+ os.sep + 'HC' + os.sep\n",
    "dir_ALS = '../wav' + os.sep + 'ALS' + os.sep\n",
    "\n",
    "DataSet_ALS = ALSDataset(dir_HC, dir_ALS)\n",
    "# print(len(DataSet_ALS))\n",
    "X_generic, y_label = DataSet_ALS.__getitem__(1220)\n",
    "\n",
    "plt.pcolor(X_generic, cmap='jet')\n",
    "print('label = ', y_label)\n",
    "fold_0_train,fold_0_test = DataSet_ALS.get_kth_fold_inds(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-fold CV procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/60883696/k-fold-cross-validation-using-dataloaders-in-pytorch\n",
    "\n",
    "def crossvalid(dataset=None, model = None, train = None, k_fold=5):    \n",
    "    \n",
    "    total_size = len(dataset)\n",
    "    y_pred = np.array(total_size)\n",
    "    y_true = np.array(total_size)\n",
    "\n",
    "    for i in range(k_fold):\n",
    "        \n",
    "        inds_train,inds_test = dataset.get_kth_fold_inds(i)                \n",
    "        \n",
    "        train_set = torch.utils.data.dataset.Subset(dataset,inds_train)\n",
    "        test_set = torch.utils.data.dataset.Subset(dataset,inds_test)\n",
    "                \n",
    "        train_loader = torch.utils.data.DataLoader(train_set, batch_size=1,\n",
    "                                          shuffle=True)  # num_workers=2\n",
    "        test_loader = torch.utils.data.DataLoader(test_set, batch_size=1,\n",
    "                                          shuffle=False)    # num_workers=2\n",
    "        \n",
    "        for layer in model.children():\n",
    "            if hasattr(layer, 'reset_parameters'):\n",
    "                layer.reset_parameters()\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        # optimizer = optim.SGD(ALS_cnn.parameters(), lr=1e-3, momentum=0.10)\n",
    "        optimizer = optim.Adam(model.parameters(),lr=1e-4)\n",
    "\n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        train(\n",
    "            n_epochs=20,\n",
    "            optimizer=optimizer,\n",
    "            model=model,\n",
    "            loss_fn=loss_fn,\n",
    "            train_loader=train_loader\n",
    "        )\n",
    "\n",
    "\n",
    "        # prediction on test set        \n",
    "        test_score = list()\n",
    "        TP_TN_sum = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            i = 0\n",
    "            for spec, label in test_loader:\n",
    "                spec = spec.float()\n",
    "                spec = spec.squeeze()\n",
    "                pred = model(spec)\n",
    "                test_score.append(pred.numpy())\n",
    "                if pred>0:\n",
    "                    pred = torch.tensor(1, dtype=torch.float64)\n",
    "                    y_pred[inds_test[i]] = 1\n",
    "                else:\n",
    "                    pred = torch.tensor(0, dtype=torch.float64)                \n",
    "                    y_pred[inds_test[i]] = 0\n",
    "                y_true[inds_test[i]] = label\n",
    "                if pred==label:\n",
    "                    TP_TN_sum +=1                \n",
    "                i +=1                 \n",
    "            print('Acc = ', TP_TN_sum/len(test_score))\n",
    "\n",
    "    acc_sk = metrics.accuracy_score(y_true, y_pred)\n",
    "    recall = metrics.recall_score(y_true, y_pred)\n",
    "    prec = metrics.precision_score(y_true, y_pred)\n",
    "    tn, fp, fn, tp = metrics.confusion_matrix(y_true, y_pred).ravel()\n",
    "    sensetivity = tp / (tp+fn)\n",
    "    specificity = tn / (tn+fp)\n",
    "    print('Final results')\n",
    "    print('Acc_sk = ', acc_sk)\n",
    "    print('Prec = ', prec)\n",
    "    print('Recall = ',recall)\n",
    "    print('Sens = ', sensetivity)\n",
    "    print('Spec = ', specificity)\n",
    "\n",
    "        # val_acc = valid(res_model,criterion,optimizer,val_loader)\n",
    "        # val_score.at[i] = val_acc\n",
    "    \n",
    "    return acc_sk\n",
    "        \n",
    "\n",
    "# train_score,val_score = crossvalid(model,criterion,optimizer,dataset=DataSet_ALS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ALS_FFNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ALS_FFNN,self).__init__() \n",
    "        # parameters\n",
    "        file = open(\"config.json\")\n",
    "        config = json.load(file)            \n",
    "        self.n_mel = config['data']['n_mel_channels']\n",
    "        self.frames_in_segment = config['data']['supra_frame_length']\n",
    "        self.fc = nn.Linear(self.n_mel*self.frames_in_segment,1)\n",
    "        nn.init.xavier_uniform_(self.fc.weight, gain=1.0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = x.view(-1,self.n_mel*self.frames_in_segment)\n",
    "        out = self.fc(out)\n",
    "        return out    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):    \n",
    "    for epoch in range(1,n_epochs+1):\n",
    "        loss_train = 0.0\n",
    "        iter = 1\n",
    "        for specs, labels in train_loader:\n",
    "            specs = specs.float()\n",
    "            outputs = model(specs)\n",
    "            outputs = outputs.squeeze()            \n",
    "            # print(outputs)\n",
    "            # print(labels)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_train += loss.item()\n",
    "            if iter%500==0:\n",
    "                print(f'{datetime.datetime.now()} Iter {iter}')\n",
    "            iter=iter + 1\n",
    "\n",
    "        # if epoch==1 or epoch%2==0:\n",
    "        print(f'{datetime.datetime.now()} Epoch {epoch}, Train loss {loss_train / len(train_loader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-12 15:15:24.001654 Epoch 1, Train loss 2.404245296802819\n",
      "2023-05-12 15:15:24.869958 Epoch 2, Train loss 0.874554066349372\n",
      "2023-05-12 15:15:25.746050 Epoch 3, Train loss 0.7885269097557132\n",
      "2023-05-12 15:15:26.660760 Epoch 4, Train loss 0.619294850156132\n",
      "2023-05-12 15:15:27.518776 Epoch 5, Train loss 0.682615871143993\n",
      "2023-05-12 15:15:28.331202 Epoch 6, Train loss 0.7157369371866507\n",
      "2023-05-12 15:15:29.224169 Epoch 7, Train loss 0.5242129157860714\n",
      "2023-05-12 15:15:30.118992 Epoch 8, Train loss 0.5832486762961843\n",
      "2023-05-12 15:15:31.008834 Epoch 9, Train loss 0.4268240641629844\n",
      "2023-05-12 15:15:31.876173 Epoch 10, Train loss 0.5139924594485441\n",
      "2023-05-12 15:15:32.727874 Epoch 11, Train loss 0.3902339525334822\n",
      "2023-05-12 15:15:33.650718 Epoch 12, Train loss 0.364496729557785\n",
      "2023-05-12 15:15:34.526573 Epoch 13, Train loss 0.3788443533240386\n",
      "2023-05-12 15:15:35.397061 Epoch 14, Train loss 0.323219198668103\n",
      "2023-05-12 15:15:36.254113 Epoch 15, Train loss 0.36431203435413567\n",
      "2023-05-12 15:15:37.163834 Epoch 16, Train loss 0.33305082643563133\n",
      "2023-05-12 15:15:38.014562 Epoch 17, Train loss 0.306314258226607\n",
      "2023-05-12 15:15:38.877880 Epoch 18, Train loss 0.26333035272445954\n",
      "2023-05-12 15:15:39.783455 Epoch 19, Train loss 0.30459441360892675\n",
      "2023-05-12 15:15:40.702251 Epoch 20, Train loss 0.30708022613234914\n"
     ]
    }
   ],
   "source": [
    "fold_0_train_inds,fold_0_test_inds = DataSet_ALS.get_kth_fold_inds(0)\n",
    "train_set = torch.utils.data.dataset.Subset(DataSet_ALS, fold_0_train_inds)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=50, shuffle=True) # num_workers=1\n",
    "\n",
    "ALS_ffnn = ALS_FFNN()\n",
    "ALS_ffnn.train()\n",
    "\n",
    "# summary(ALS_cnn,(64*86))\n",
    "# optimizer = optim.SGD(ALS_cnn.parameters(), lr=1e-3, momentum=0.10)\n",
    "optimizer = optim.Adam(ALS_ffnn.parameters(),lr=1e-4)\n",
    "\n",
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "# pos_weight = torch.ones([1])\n",
    "# loss_fn = nn.BCEWithLogitsLoss(pos_weight = pos_weight)\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs=20,\n",
    "    optimizer=optimizer,\n",
    "    model=ALS_ffnn,\n",
    "    loss_fn=loss_fn,\n",
    "    train_loader=train_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALS_ffnn.eval()\n",
    "\n",
    "test_set = torch.utils.data.dataset.Subset(DataSet_ALS, fold_0_test_inds)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=1, shuffle=False) # num_workers=1\n",
    "\n",
    "test_score = list()\n",
    "TP_TN_sum = 0\n",
    "with torch.no_grad():\n",
    "    for spec, label in test_loader:\n",
    "        spec = spec.float()\n",
    "        spec = spec.squeeze()\n",
    "        pred = ALS_ffnn(spec)\n",
    "        test_score.append(pred.numpy())\n",
    "        if pred>0:\n",
    "            pred = torch.tensor(1, dtype=torch.float64)\n",
    "        else:\n",
    "            pred = torch.tensor(0, dtype=torch.float64)\n",
    "            # print('Neg example')    \n",
    "        if pred==label:\n",
    "            TP_TN_sum +=1\n",
    "\n",
    "print('Acc = ', TP_TN_sum/len(test_score))\n",
    "\n",
    "test_score = np.array(test_score).squeeze()\n",
    "plt.plot(test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with 5-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Target size (torch.Size([1])) must be the same as input size (torch.Size([]))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\GitHub\\diploma\\pytorch\\cnn_ALS_classifier.ipynb Ячейка 20\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/GitHub/diploma/pytorch/cnn_ALS_classifier.ipynb#X25sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m DataSet_ALS \u001b[39m=\u001b[39m ALSDataset(dir_HC, dir_ALS)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/GitHub/diploma/pytorch/cnn_ALS_classifier.ipynb#X25sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m ALS_ffnn \u001b[39m=\u001b[39m ALS_FFNN()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/GitHub/diploma/pytorch/cnn_ALS_classifier.ipynb#X25sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m crossvalid(dataset\u001b[39m=\u001b[39;49mDataSet_ALS, model \u001b[39m=\u001b[39;49m ALS_ffnn, train \u001b[39m=\u001b[39;49m training_loop, k_fold\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m)\n",
      "\u001b[1;32md:\\GitHub\\diploma\\pytorch\\cnn_ALS_classifier.ipynb Ячейка 20\u001b[0m in \u001b[0;36mcrossvalid\u001b[1;34m(dataset, model, train, k_fold)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GitHub/diploma/pytorch/cnn_ALS_classifier.ipynb#X25sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam(ALS_ffnn\u001b[39m.\u001b[39mparameters(),lr\u001b[39m=\u001b[39m\u001b[39m1e-4\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GitHub/diploma/pytorch/cnn_ALS_classifier.ipynb#X25sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m loss_fn \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mBCEWithLogitsLoss()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/GitHub/diploma/pytorch/cnn_ALS_classifier.ipynb#X25sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m train(\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GitHub/diploma/pytorch/cnn_ALS_classifier.ipynb#X25sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     n_epochs\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GitHub/diploma/pytorch/cnn_ALS_classifier.ipynb#X25sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     optimizer\u001b[39m=\u001b[39;49moptimizer,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GitHub/diploma/pytorch/cnn_ALS_classifier.ipynb#X25sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     model\u001b[39m=\u001b[39;49mALS_ffnn,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GitHub/diploma/pytorch/cnn_ALS_classifier.ipynb#X25sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     loss_fn\u001b[39m=\u001b[39;49mloss_fn,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GitHub/diploma/pytorch/cnn_ALS_classifier.ipynb#X25sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     train_loader\u001b[39m=\u001b[39;49mtrain_loader\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GitHub/diploma/pytorch/cnn_ALS_classifier.ipynb#X25sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GitHub/diploma/pytorch/cnn_ALS_classifier.ipynb#X25sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39m# prediction on test set        \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GitHub/diploma/pytorch/cnn_ALS_classifier.ipynb#X25sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m test_score \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m()\n",
      "\u001b[1;32md:\\GitHub\\diploma\\pytorch\\cnn_ALS_classifier.ipynb Ячейка 20\u001b[0m in \u001b[0;36mtraining_loop\u001b[1;34m(n_epochs, optimizer, model, loss_fn, train_loader)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/GitHub/diploma/pytorch/cnn_ALS_classifier.ipynb#X25sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m outputs \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39msqueeze()            \n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/GitHub/diploma/pytorch/cnn_ALS_classifier.ipynb#X25sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# print(outputs)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GitHub/diploma/pytorch/cnn_ALS_classifier.ipynb#X25sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# print(labels)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/GitHub/diploma/pytorch/cnn_ALS_classifier.ipynb#X25sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(outputs, labels)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GitHub/diploma/pytorch/cnn_ALS_classifier.ipynb#X25sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GitHub/diploma/pytorch/cnn_ALS_classifier.ipynb#X25sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:713\u001b[0m, in \u001b[0;36mBCEWithLogitsLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    712\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 713\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbinary_cross_entropy_with_logits(\u001b[39minput\u001b[39;49m, target,\n\u001b[0;32m    714\u001b[0m                                               \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[0;32m    715\u001b[0m                                               pos_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpos_weight,\n\u001b[0;32m    716\u001b[0m                                               reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\functional.py:3130\u001b[0m, in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[1;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[0;32m   3127\u001b[0m     reduction_enum \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mget_enum(reduction)\n\u001b[0;32m   3129\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (target\u001b[39m.\u001b[39msize() \u001b[39m==\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize()):\n\u001b[1;32m-> 3130\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTarget size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) must be the same as input size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(target\u001b[39m.\u001b[39msize(), \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize()))\n\u001b[0;32m   3132\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mbinary_cross_entropy_with_logits(\u001b[39minput\u001b[39m, target, weight, pos_weight, reduction_enum)\n",
      "\u001b[1;31mValueError\u001b[0m: Target size (torch.Size([1])) must be the same as input size (torch.Size([]))"
     ]
    }
   ],
   "source": [
    "dir_HC = '../wav'+ os.sep + 'HC' + os.sep\n",
    "dir_ALS = '../wav' + os.sep + 'ALS' + os.sep\n",
    "\n",
    "DataSet_ALS = ALSDataset(dir_HC, dir_ALS)\n",
    "ALS_ffnn = ALS_FFNN()\n",
    "\n",
    "crossvalid(dataset=DataSet_ALS, model = ALS_ffnn, train = training_loop, k_fold=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 86])\n",
      "torch.Size([1, 5504])\n",
      "torch.Size([1, 10])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# Test 1\n",
    "test_img = torch.rand(64,86)\n",
    "test_img_row = test_img.view(-1,64*86)\n",
    "print(test_img.shape)\n",
    "print(test_img_row.shape)\n",
    "\n",
    "# Test 2\n",
    "test_img = torch.rand(1,10)\n",
    "test_img_row = test_img.squeeze()\n",
    "print(test_img.shape)\n",
    "print(test_img_row.shape)\n",
    "\n",
    "#Test 3\n",
    "# print(torch.sigmoid(torch.tensor(-1)), torch.sigmoid(torch.tensor(0)), torch.sigmoid(torch.tensor(1)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV-Fold selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 26  97 117  85  53  86 127  92 102  27  32  72  39]\n",
      "Fold 1 HC age mean = 52.43, mens = 3, womens = 4\n",
      "Fold 1 ALS age mean = 56.83, mens = 3, womens = 3\n",
      "[ 28 123  63  99  81 125 129  68  21  55  48  78  42]\n",
      "Fold 2 HC age mean = 53.71, mens = 3, womens = 4\n",
      "Fold 2 ALS age mean = 58.33, mens = 4, womens = 2\n",
      "[ 61  16 115 109 111   2 107  46  84  64  52  24  76]\n",
      "Fold 3 HC age mean = 55.43, mens = 2, womens = 5\n",
      "Fold 3 ALS age mean = 59.67, mens = 4, womens = 2\n",
      "[ 77   6  65  24 131 119  25  94  62  28   8  98  22]\n",
      "Fold 4 HC age mean = 53.00, mens = 3, womens = 3\n",
      "Fold 4 ALS age mean = 60.86, mens = 5, womens = 2\n",
      "[ 49  89   4  42 113 121  96  20  58  80  31 100]\n",
      "Fold 5 HC age mean = 54.17, mens = 2, womens = 4\n",
      "Fold 5 ALS age mean = 61.00, mens = 4, womens = 2\n"
     ]
    }
   ],
   "source": [
    "file_name =  'HC_ALS_table.xlsx' # path to file + file name\n",
    "\n",
    "xls_file = pd.ExcelFile(file_name)\n",
    "# print(xls_file.sheet_names)\n",
    "\n",
    "# https://stackoverflow.com/questions/17071871/how-do-i-select-rows-from-a-dataframe-based-on-column-values\n",
    "\n",
    "df_HC = xls_file.parse('Control')\n",
    "df_ALS = xls_file.parse('ALS')\n",
    "# print(df_HC['fold']==1)\n",
    "for fold_num in range(1,6):\n",
    "    HC_fold = df_HC.loc[df_HC['fold']==fold_num]\n",
    "    ALS_fold = df_ALS.loc[df_ALS['fold']==fold_num]\n",
    "    # print(HC_fold)\n",
    "    # print(ALS_fold)\n",
    "    print(np.hstack((HC_fold['Subject code'].to_numpy(), ALS_fold['Subject code'].to_numpy())))\n",
    "    HC_age = (float)(HC_fold.loc[:, 'Age'].mean())\n",
    "    HC_m = (HC_fold.loc[HC_fold['Sex']=='m'])\n",
    "    HC_f = (HC_fold.loc[HC_fold['Sex']=='f'])\n",
    "    ALS_age = (float)(ALS_fold.loc[:, 'Age'].mean())\n",
    "    ALS_m = (ALS_fold.loc[ALS_fold['Sex']=='m'])\n",
    "    ALS_f = (ALS_fold.loc[ALS_fold['Sex']=='f'])\n",
    "    print(f'Fold {fold_num} HC age mean = {HC_age:.2f}, mens = {len(HC_m.index)}, womens = {len(HC_f.index)}')\n",
    "    print(f'Fold {fold_num} ALS age mean = {ALS_age:.2f}, mens = {len(ALS_m.index)}, womens = {len(ALS_f.index)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a42ccb73e7d9bfdf27e036f1d2b8b681e55fc0743cc5586bc2474d4a60f4b886"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
